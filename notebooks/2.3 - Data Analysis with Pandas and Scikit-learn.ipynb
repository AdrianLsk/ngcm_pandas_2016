{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "## Regression modeling\n",
    "\n",
    "A general, primary goal of many statistical data analysis tasks is to relate the influence of one variable on another. For example, we may wish to know how different medical interventions influence the incidence or duration of disease, or perhaps a how baseball player's performance varies as a function of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy.optimize import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x':np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0]),\n",
    "                    'y':np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])})\n",
    "data.plot.scatter('x', 'y', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a model to characterize the relationship between $X$ and $Y$, recognizing that additional factors other than $X$ (the ones we have measured or are interested in) may influence the response variable $Y$.\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = f(x_i) + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f$ is some function, for example a linear function:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\epsilon_i$ accounts for the difference between the observed response $y_i$ and its prediction from the model $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$. This is sometimes referred to as **process uncertainty**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to select $\\beta_0, \\beta_1$ so that the difference between the predictions and the observations is zero, but this is not usually possible. Instead, we choose a reasonable criterion: ***the smallest sum of the squared differences between $\\hat{y}$ and $y$***.\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$R^2 = \\sum_i (y_i - [\\beta_0 + \\beta_1 x_i])^2 = \\sum_i \\epsilon_i^2 $$  \n",
    "</div>\n",
    "\n",
    "Squaring serves two purposes: (1) to prevent positive and negative values from cancelling each other out and (2) to strongly penalize large deviations. Whether the latter is a good thing or not depends on the goals of the analysis.\n",
    "\n",
    "In other words, we will select the parameters that minimize the squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_of_squares = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_of_squares([0,1], data.x, data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1 = fmin(sum_of_squares, [0,1], args=(data.x, data.y))\n",
    "b0,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = data.plot.scatter('x', 'y', s=50)\n",
    "axes.plot([0,10], [b0, b0+b1*10])\n",
    "axes.set_xlim(2, 9)\n",
    "axes.set_ylim(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = data.plot.scatter('x', 'y', s=50)\n",
    "axes.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = data.plot.scatter('x', 'y', s=50)\n",
    "axes.plot([0,10], [b0, b0+b1*10])\n",
    "for i,(xi, yi) in data.iterrows():\n",
    "    axes.plot([xi]*2, [yi, b0+b1*xi], 'k:')\n",
    "axes.set_xlim(2, 9)\n",
    "axes.set_ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the sum of squares is not the only criterion we can use; it is just a very popular (and successful) one. For example, we can try to minimize the sum of absolute differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_of_absval = lambda theta, x, y: np.sum(np.abs(y - theta[0] - theta[1]*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1 = fmin(sum_of_absval, [0,1], args=(data.x,data.y))\n",
    "print('\\nintercept: {0:.2}, slope: {1:.2}'.format(b0,b1))\n",
    "axes = data.plot.scatter('x', 'y', s=50)\n",
    "axes.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not restricted to a straight-line regression model; we can represent a curved relationship between our variables by introducing **polynomial** terms. For example, a cubic model:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_squares_quad = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x - theta[2]*(x**2)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1,b2 = fmin(sum_squares_quad, [1,1,-1], args=(data.x, data.y))\n",
    "print('\\nintercept: {0:.2}, x: {1:.2}, x2: {2:.2}'.format(b0,b1,b2))\n",
    "axes = data.plot.scatter('x', 'y', s=50)\n",
    "xvals = np.linspace(0, 10, 100)\n",
    "axes.plot(xvals, b0 + b1*xvals + b2*(xvals**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although polynomial model characterizes a nonlinear relationship, it is a linear problem in terms of estimation. That is, the regression model $f(y | x)$ is linear in the parameters.\n",
    "\n",
    "For some data, it may be reasonable to consider polynomials of order>2. For example, consider the relationship between the number of home runs a baseball player hits and the number of runs batted in (RBI) they accumulate; clearly, the relationship is positive, but we may not expect a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_squares_cubic = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x - theta[2]*(x**2) \n",
    "                                  - theta[3]*(x**3)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine = pd.read_table(\"../data/wine.dat\", sep='\\s+')\n",
    "\n",
    "attributes = ['Grape',\n",
    "            'Alcohol',\n",
    "            'Malic acid',\n",
    "            'Ash',\n",
    "            'Alcalinity of ash',\n",
    "            'Magnesium',\n",
    "            'Total phenols',\n",
    "            'Flavanoids',\n",
    "            'Nonflavanoid phenols',\n",
    "            'Proanthocyanins',\n",
    "            'Color intensity',\n",
    "            'Hue',\n",
    "            'OD280/OD315 of diluted wines',\n",
    "            'Proline']\n",
    "\n",
    "wine.columns = attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Total phenols', 'Flavanoids', c='red')\n",
    "phenols, flavanoids = wine[['Total phenols', 'Flavanoids']].T.values\n",
    "b0,b1,b2,b3 = fmin(sum_squares_cubic, [0,1,-1,0], args=(phenols, flavanoids))\n",
    "xvals = np.linspace(-2, 2)\n",
    "axes.plot(xvals, b0 + b1*xvals + b2*(xvals**2) + b3*(xvals**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we need not fit least squares models by hand because they are implemented generally in packages such as [`scikit-learn`](http://scikit-learn.org/) and [`statsmodels`](https://github.com/statsmodels/statsmodels/). For example, `scikit-learn` package implements least squares models in its `LinearRegression` class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `Scikit-learn`\n",
    "\n",
    "The `scikit-learn` package is an open-source library that provides a robust set of machine learning algorithms for Python. It is built upon the core Python scientific stack (*i.e.* NumPy, SciPy, Cython), and has a simple, consistent API, making it useful for a wide range of statistical learning applications.\n",
    "\n",
    "![scikit learn methods](images/sklearn.png)\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is about coding programs that automatically adjust their performance from exposure to information encoded in data. This learning is achieved via **tunable parameters** that are automatically adjusted according to performance criteria.\n",
    "\n",
    "Machine Learning can be considered a subfield of Artificial Intelligence (AI).\n",
    "\n",
    "There are three major classes of ML:\n",
    "\n",
    "**Supervised learning**\n",
    ": Algorithms which learn from a training set of *labeled* examples (exemplars) to generalize to the set of all possible inputs. Examples of supervised learning include regression and support vector machines.\n",
    "\n",
    "**Unsupervised learning**\n",
    ": Algorithms which learn from a training set of *unlableled* examples, using the features of the inputs to categorize inputs together according to some statistical criteria. Examples of unsupervised learning include k-means clustering and kernel density estimation.\n",
    "\n",
    "**Reinforcement learning**\n",
    ": Algorithms that learn via reinforcement from a *critic* that provides information on the quality of a solution, but not on how to improve it. Improved solutions are achieved by iteratively exploring the solution space. We will not cover RL in this workshop.\n",
    "\n",
    "## Representing Data in `scikit-learn`\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given scikit-learn **estimator** object named `model`, several methods are available. Irrespective of the type of **estimator**, there will be a `fit` method:\n",
    "\n",
    "- `model.fit` : fit training data. For supervised learning applications, this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`). For unsupervised learning applications, this accepts only a single argument, the data `X` (e.g. `model.fit(X)`).\n",
    "\n",
    "> During the fitting process, the state of the **estimator** is stored in attributes of the estimator instance named with a trailing underscore character (\\_). For example, the sequence of regression trees `sklearn.tree.DecisionTreeRegressor` is stored in `estimators_` attribute.\n",
    "\n",
    "The **predictor** interface extends the notion of an estimator by adding a `predict` method that takes an array `X_test` and produces predictions based on the learned parameters of the estimator. In the case of supervised learning estimators, this method typically returns the predicted labels or values computed by the model. Some unsupervised learning estimators may also implement the predict interface, such as k-means, where the predicted values are the cluster labels.\n",
    "\n",
    "**supervised estimators** are expected to have the following methods:\n",
    "\n",
    "- `model.predict` : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`), and returns the learned label for each object in the array.\n",
    "- `model.predict_proba` : For classification problems, some estimators also provide this method, which returns the probability that a new observation has each categorical label. In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "- `model.score` : for classification or regression problems, most (all?) estimators implement a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "\n",
    "Since it is common to modify or ﬁlter data before feeding it to a learning algorithm, some estimators in the library implement a **transformer** interface which deﬁnes a `transform` method. It takes as input some new data `X_test` and yields as output a transformed version. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n",
    "\n",
    "**unsupervised estimators** will always have these methods:\n",
    "\n",
    "- `model.transform` : given an unsupervised model, transform new data into the new basis. This also accepts one argument `X_new`, and returns the new representation of the data based on the unsupervised model.\n",
    "- `model.fit_transform` : some estimators implement this method, which more efficiently performs a fit and a transform on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn` interface\n",
    "\n",
    "All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: \n",
    "\n",
    "* **estimator** interface for building and ﬁtting models\n",
    "* **predictor** interface for making predictions\n",
    "* **transformer** interface for converting data.\n",
    "\n",
    "The estimator interface is at the core of the library. It deﬁnes instantiation mechanisms of objects and exposes a fit method for learning a model from training data. All supervised and unsupervised learning algorithms (*e.g.*, for classiﬁcation, regression or clustering) are oﬀered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods. For example, a typical **estimator** follows this template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model to data X (and y)\"\"\"\n",
    "        self.some_attribute = self.some_fitting_method(X, y)\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make prediction based on passed features\"\"\"\n",
    "        pred = self.make_prediction(X_test)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly-used statistical method in scikit-learn is the Principal Components Analysis, which is implemented in the `PCA` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "wine_predictors = wine[wine.columns[1:]]\n",
    "pca = PCA(n_components=2, whiten=True).fit(wine_predictors)\n",
    "X_pca = pd.DataFrame(pca.transform(wine_predictors), columns=['Component 1' , 'Component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = X_pca.plot.scatter(x='Component 1' , y='Component 2', c=wine.Grape, cmap='Accent')\n",
    "var_explained = pca.explained_variance_ratio_ * 100\n",
    "axes.set_xlabel('First Component: {0:.1f}%'.format(var_explained[0]))\n",
    "axes.set_ylabel('Second Component: {0:.1f}%'.format(var_explained[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there is a `LinearRegression` class we can use for our regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "straight_line = linear_model.LinearRegression()\n",
    "straight_line.fit(data.x.reshape(-1, 1), data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "straight_line.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = data.plot.scatter('x', 'y', s=50)\n",
    "axes.plot(data.x, straight_line.predict(data.x[:, np.newaxis]), color='red',\n",
    "         linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more general regression model building, its helpful to use a tool for describing statistical models, called `patsy`. With `patsy`, it is easy to specify the desired combinations of variables for any particular analysis, using an \"R-like\" syntax. `patsy` parses the formula string, and uses it to construct the approriate *design matrix* for the model.\n",
    "\n",
    "For example, the quadratic model specified by hand above can be coded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "X = dmatrix('phenols + I(phenols**2) + I(phenols**3)')\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dmatrix` function returns the design matrix, which can be passed directly to the `LinearRegression` fitting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_line = linear_model.LinearRegression(fit_intercept=False)\n",
    "poly_line.fit(X, flavanoids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_line.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Total phenols', 'Flavanoids', c='red')\n",
    "axes.plot(xvals, poly_line.predict(dmatrix('xvals + I(xvals**2) + I(xvals**3)')), color='blue',\n",
    "         linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Fitting a line to the relationship between two variables using the least squares approach is sensible when the variable we are trying to predict is continuous, but what about when the data are dichotomous?\n",
    "\n",
    "- male/female\n",
    "- pass/fail\n",
    "- died/survived\n",
    "\n",
    "Let's consider the problem of predicting survival in the Titanic disaster, based on our available information. For example, lets say that we want to predict survival as a function of the fare paid for the journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_excel(\"../data/titanic.xls\", \"titanic\")\n",
    "titanic.name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.02, size=len(titanic))\n",
    "axes = (titanic.assign(logfar=np.log(titanic.fare), surv_jit=titanic.survived + jitter)\n",
    "         .plot.scatter('logfar', 'surv_jit', alpha=0.3))\n",
    "axes.set_yticks([0,1])\n",
    "axes.set_ylabel('survived')\n",
    "axes.set_xlabel('log(fare)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added random jitter on the y-axis to help visualize the density of the points, and have plotted fare on the log scale.\n",
    "\n",
    "Clearly, fitting a line through this data makes little sense, for several reasons. First, for most values of the predictor variable, the line would predict values that are not zero or one. Second, it would seem odd to choose least squares (or similar) as a criterion for selecting the best line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.log(titanic.fare[titanic.fare>0])\n",
    "y = titanic.survived[titanic.fare>0]\n",
    "betas_titanic = fmin(sum_of_squares, [1,1], args=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.02, size=len(titanic))\n",
    "axes = (titanic.assign(logfar=np.log(titanic.fare), surv_jit=titanic.survived + jitter)\n",
    "         .plot.scatter('logfar', 'surv_jit', alpha=0.3))\n",
    "axes.set_yticks([0,1])\n",
    "axes.set_ylabel('survived')\n",
    "axes.set_xlabel('log(fare)')\n",
    "\n",
    "axes.plot([0,7], [betas_titanic[0], betas_titanic[0] + betas_titanic[1]*7.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at this data, we can see that for most values of `fare`, there are some individuals that survived and some that did not. However, notice that the cloud of points is denser on the \"survived\" (`y=1`) side for larger values of fare than on the \"died\" (`y=0`) side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic model\n",
    "\n",
    "Rather than model the binary outcome explicitly, it makes sense instead to model the *probability* of death or survival in a **stochastic** model. Probabilities are measured on a continuous [0,1] scale, which may be more amenable for prediction using a regression line. We need to consider a different probability model for this exerciese however; let's consider the **Bernoulli** distribution as a generative model for our data:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$f(y|p) = p^{y} (1-p)^{1-y}$$ \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y = \\{0,1\\}$ and $p \\in [0,1]$. So, this model predicts whether $y$ is zero or one as a function of the probability $p$. Notice that when $y=1$, the $1-p$ term disappears, and when $y=0$, the $p$ term disappears.\n",
    "\n",
    "So, the model we want to fit should look something like this:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$p_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since $p$ is constrained to be between zero and one, it is easy to see where a linear (or polynomial) model might predict values outside of this range. We can modify this model sligtly by using a **link function** to transform the probability to have an unbounded range on a new scale. Specifically, we can use a **logit transformation** as our link function:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$\\text{logit}(p) = \\log\\left[\\frac{p}{1-p}\\right] = x$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of $p/(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit = lambda p: np.log(p/(1.-p))\n",
    "unit_interval = np.linspace(0,1)\n",
    "plt.plot(unit_interval/(1-unit_interval), unit_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the logit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(logit(unit_interval), unit_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of the logit transformation is:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$p = \\frac{1}{1 + \\exp(-x)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invlogit = lambda x: 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now our model is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$\\text{logit}(p_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this model using maximum likelihood. Our likelihood, again based on the Bernoulli model is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$L(y|p) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, on the log scale is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$l(y|p) = \\sum_{i=1}^n y_i \\log(p_i) + (1-y_i)\\log(1-p_i)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily implement this in Python, keeping in mind that `fmin` minimizes, rather than maximizes functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invlogit = lambda x: 1/(1 + np.exp(-x))\n",
    "\n",
    "def logistic_like(theta, x, y):\n",
    "    \n",
    "    p = invlogit(theta[0] + theta[1] * x)\n",
    "    \n",
    "    # Return negative of log-likelihood\n",
    "    return -np.sum(y * np.log(p) + (1-y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove null values from variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = titanic[titanic.fare.notnull()][['fare', 'survived']].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0, b1 = fmin(logistic_like, [0.5,0], args=(x,y))\n",
    "b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.02, size=len(titanic))\n",
    "axes = (titanic.assign(surv_jit=titanic.survived + jitter)\n",
    "         .plot.scatter('fare', 'surv_jit', alpha=0.3))\n",
    "axes.set_yticks([0,1])\n",
    "axes.set_ylabel('survived')\n",
    "axes.set_xlabel('log(fare)')\n",
    "\n",
    "xvals = np.linspace(0, 600)\n",
    "axes.plot(xvals, invlogit(b0+b1*xvals),c='red')\n",
    "axes.set_xlim(0, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our least squares model, we can easily fit logistic regression models in `scikit-learn`, in this case using the `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X0 = x[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X0, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` model in scikit-learn employs a regularization coefficient `C`, which defaults to 1. The amount of regularization is lower with larger values of C.\n",
    "\n",
    "Regularization penalizes the values of regression coefficients, while smaller ones let the coefficients range widely. Scikit-learn includes two penalties: a **l2** penalty which penalizes the sum of the squares of the coefficients (the default), and a **l1** penalty which penalizes the sum of the absolute values.\n",
    "\n",
    "The reason for doing regularization is to let us to include more covariates than our data might otherwise allow. We only have a few coefficients, so we will set `C` to a large value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrmod = LogisticRegression(C=1000)\n",
    "lrmod.fit(X_train, y_train)\n",
    "\n",
    "pred_train = lrmod.predict(X_train)\n",
    "pred_test = lrmod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_train, pred_train, \n",
    "            rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, pred_test, \n",
    "            rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrmod.fit(x[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrmod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: multivariate logistic regression\n",
    "\n",
    "Which other variables might be relevant for predicting the probability of surviving the Titanic? Generalize the model likelihood to include 2 or 3 other covariates from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Uncertainty: Bootstrapping\n",
    "\n",
    "Resampling is the process of repeatedly **drawing subsamples** from a training dataset, and fitting a model to each sample with the goal of discovering additional properties or information about the model. For example, in a regression modeling context, we can fit a particular regression model to each sample, and observe **how the fits vary** among the samples. \n",
    "\n",
    "We will introduce **bootstrapping**, an important resampling method that is used in statistical and machine learning applications, particularly for **assessing** models and estimating the **precision** of parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Parametric inference can be **non-robust**:\n",
    "\n",
    "* inaccurate if parametric assumptions are violated\n",
    "* if we rely on asymptotic results, we may not achieve an acceptable level of accuracy\n",
    "\n",
    "Parmetric inference can be **difficult**:\n",
    "\n",
    "* derivation of sampling distribution may not be possible\n",
    "\n",
    "An alternative is to estimate the sampling distribution of a statistic *empirically* without making assumptions about the form of the population.\n",
    "\n",
    "We have seen this already with the kernel density estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-parametric Bootstrap\n",
    "\n",
    "The bootstrap is a resampling method discovered by [Brad Efron](http://www.jstor.org/discover/10.2307/2958830?uid=3739568&uid=2&uid=4&uid=3739256&sid=21102342537691) that allows one to approximate the true sampling distribution of a dataset, and thereby obtain estimates of the mean and variance of the distribution.\n",
    "\n",
    "Bootstrap sample:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$S_1^* = \\{x_{11}^*, x_{12}^*, \\ldots, x_{1n}^*\\}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S_i^*$ is a sample of size $n$, **with** replacement.\n",
    "\n",
    "In Python, we have already seen sampling. If we want to use NumPy for this, we can permute the column of names to obtain a sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We regard S as an \"estimate\" of population P\n",
    "\n",
    "> population : sample :: sample : bootstrap sample\n",
    "\n",
    "The idea is to generate replicate bootstrap samples:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$S^* = \\{S_1^*, S_2^*, \\ldots, S_R^*\\}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute statistic $t$ (estimate) for each bootstrap sample:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$T_i^* = t(S^*)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bootstrap some confidence intervals for our logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "R = 1000\n",
    "boot_samples = np.empty((n, len(lrmod.coef_[0])))\n",
    "\n",
    "for i in np.arange(R):\n",
    "    boot_ind = np.random.randint(0, len(X0), len(X0))\n",
    "    y_i, X_i = y[boot_ind], X0[boot_ind]\n",
    "    \n",
    "    lrmod_i = LogisticRegression(C=1000)\n",
    "    lrmod_i.fit(X_i, y_i)\n",
    "\n",
    "    boot_samples[i] = lrmod_i.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Percentile Intervals\n",
    "\n",
    "An attractive feature of bootstrap statistics is the ease with which you can obtain an estimate of *uncertainty* for a given statistic. We simply use the empirical quantiles of the bootstrapped statistics to obtain percentiles corresponding to a confidence interval of interest.\n",
    "\n",
    "This employs the *ordered* bootstrap replicates:\n",
    "\n",
    "$$T_{(1)}^*, T_{(2)}^*, \\ldots, T_{(R)}^*$$\n",
    "\n",
    "Simply extract the $100(\\alpha/2)$ and $100(1-\\alpha/2)$ percentiles:\n",
    "\n",
    "$$T_{[(R+1)\\alpha/2]}^* \\lt \\theta \\lt T_{[(R+1)(1-\\alpha/2)]}^*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_samples.sort(axis=0)\n",
    "boot_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_samples[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_interval = boot_samples[[25, 975], :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrmod.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have estimated the expectation of the bootstrapped statistics, we can estimate the **bias** of T:\n",
    "\n",
    "$$\\hat{B}^* = \\bar{T}^* - T$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_samples.mean() - lrmod.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_var = ((boot_samples - boot_samples.mean()) ** 2).sum() / (R-1)\n",
    "boot_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap error\n",
    "\n",
    "There are two sources of error in bootstrap estimates:\n",
    "\n",
    "1. **Sampling error** from the selection of $S$.\n",
    "2. **Bootstrap error** from failing to enumerate all possible bootstrap samples.\n",
    "\n",
    "For the sake of accuracy, it is prudent to choose at least R=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cervical dystonia bootstrap estimates\n",
    "\n",
    "Use bootstrapping to estimate the mean of one of the treatment groups, and calculate percentile intervals for the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupvervised Learning: Clustering\n",
    "\n",
    "Clustering is a class of unsupervised learning methods that associates observations according to some specified measure of similarity (e.g. Euclidean distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Algorithm\n",
    "\n",
    "The K-means clustering algorithm associates each point $x_i$ in a set of input points $\\{x_1, x_2, \\ldots, x_m\\}$ to $K$ clusters. Each cluster is specified by a **centroid** that is the average location of all the points in the cluster. The algorithm proceeds iteratively from arbitrary centroid locations, updating the membership of each point according to minimum distance, then updating the centroid location based on the new cluster membership. \n",
    "\n",
    "The algorithm will have converged when the assignment of points to centroids does not change with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "1. Initialize cluster centroids:\n",
    "\n",
    "    $$\\mu^{(0)}_1, \\ldots, \\mu^{(0)}_k \\in \\mathbb{R}^n$$\n",
    "\n",
    "2. Iterate until converged:\n",
    "\n",
    "    a. Set $c_i = \\text{argmin}_j || x_i - \\mu_j^{(s)} ||$\n",
    "    \n",
    "    b. Update centroids:\n",
    "    \n",
    "    $$\\mu_j^{(s+1)} = \\frac{\\sum_{i=1}^m I[c_i = j] x_i}{\\sum_{i=1}^m I[c_i = j]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means algorithm is simply a Gaussian mixture model with two restrictions: \n",
    "\n",
    "1. the covariance matrix is spherical: \n",
    "\n",
    "    $$\\Sigma_k = \\sigma I_D$$\n",
    "\n",
    "2. the mixture weights are fixed:\n",
    "\n",
    "    $$\\pi_k = \\frac{1}{K}$$\n",
    "\n",
    "Hence, we are only interested in locating the appropriate centroid of the clusters. This serves to speed computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the distortion function:\n",
    "\n",
    "$$J(c,\\mu) = \\sum_{i]1}^m ||x_i - \\mu_{c_i}||$$\n",
    "\n",
    "which gets smaller at every iteration. So, k-means is coordinate ascent on $J(c,\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    "\n",
    "To check whether a chosen $k$ is reasonable, one approach is to compare the distances between the centroids to the mean distance bewween each data point and their assigned centroid. A good fit involves relatively large inter-centroid distances. \n",
    "\n",
    "The appropriate value for k (the number of clusters) may depend on the goals of the analysis, or it may be chosen algorithmically, using an optimization procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,x in enumerate(X.columns):\n",
    "    print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.plot.scatter('Flavanoids', 'Malic acid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.plot.scatter('Flavanoids', 'Malic acid', c=np.array(list('rgbc'))[wine.Grape-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with $k=3$, arbitrarily assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = (-1, 2), (-1, -1), (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Flavanoids', 'Malic acid')\n",
    "axes.scatter(*np.transpose(centroids), c='r', lw=3, marker='+', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `cdist` from SciPy to calculate the distances from each point to each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distances = cdist(centroids, wine[['Flavanoids', 'Malic acid']])\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the initial assignment to centroids by picking the minimum distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = distances.argmin(axis=0)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Flavanoids', 'Malic acid', c=np.array(list('rgbc'))[labels])\n",
    "axes.scatter(*np.transpose(centroids), c='r', marker='+', lw=3, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-assign the centroid locations based on the means of the current members' locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_centroids = [wine.loc[labels==i, ['Flavanoids', 'Malic acid']].values.mean(0) for i in range(len(centroids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Flavanoids', 'Malic acid', c=np.array(list('rgbc'))[labels])\n",
    "axes.scatter(*np.transpose(new_centroids), c='r', marker='+', s=100, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we simply iterate these steps until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = new_centroids\n",
    "iterations = 200\n",
    "\n",
    "for _ in range(iterations):\n",
    "    distances = cdist(centroids, wine[['Flavanoids', 'Malic acid']])\n",
    "    labels = distances.argmin(axis=0)\n",
    "    centroids = [wine.loc[labels==i, ['Flavanoids', 'Malic acid']].values.mean(0) for i in range(len(centroids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Flavanoids', 'Malic acid', c=np.array(list('rgbc'))[labels])\n",
    "axes.scatter(*np.transpose(centroids), c='r', marker='+', s=100, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means using `scikit-learn`\n",
    "\n",
    "The `scikit-learn` package includes a `KMeans` class for flexibly fitting K-means models. It includes additional features, such as initialization options and the ability to set the convergence tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(1)\n",
    "\n",
    "# Instantiate model\n",
    "kmeans = KMeans(n_clusters=3, random_state=rng)\n",
    "# Fit model\n",
    "kmeans.fit(wine[['Flavanoids', 'Malic acid']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can retrieve the labels and cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot should look very similar to the one we fit by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = wine.plot.scatter('Flavanoids', 'Malic acid', c=np.array(list('rgbc'))[labels])\n",
    "axes.scatter(*kmeans.cluster_centers_.T, c='r', marker='+', s=100, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: more wine\n",
    "\n",
    "Pick two other wine variables, and see how well they cluster relative to the true classes (grapes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "T. Hastie, R. Tibshirani and J. Friedman. (2009) [Elements of Statistical Learning: Data Mining, Inference, and Prediction](http://statweb.stanford.edu/~tibs/ElemStatLearn/), second edition. Springer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
